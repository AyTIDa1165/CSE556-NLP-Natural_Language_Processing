{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BartTokenizer, BartModel, RobertaTokenizer, RobertaForSequenceClassification\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_index(tokens, start):\n",
    "    char_index = 0\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        token_start = char_index\n",
    "        token_end = char_index + len(token)\n",
    "        \n",
    "        if token_start <= start and start <= token_end:\n",
    "            return i\n",
    "        \n",
    "        char_index = token_end + 1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_file, output_file):  \n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for item in data:\n",
    "        sentence = item['sentence']\n",
    "        tokens = sentence.split()\n",
    "        aspect_terms = item['aspect_terms']\n",
    "\n",
    "        for aspect_term in aspect_terms:\n",
    "            start = int(aspect_term['from'])\n",
    "            end = int(aspect_term['to'])\n",
    "            index = get_token_index(tokens, start)\n",
    "\n",
    "            preprocessed_item = {\n",
    "                'tokens': tokens,\n",
    "                'polarity': aspect_term['polarity'],\n",
    "                'aspect_term': aspect_term['term'].split(),\n",
    "                'index': index\n",
    "            }\n",
    "            preprocessed_data.append(preprocessed_item)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(preprocessed_data, out_file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "preprocessing('train.json', 'train_task_2.json')\n",
    "preprocessing('val.json', 'val_task_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, data, label_map, bert_model=\"bert-base-uncased\", max_len_sentence=35, max_len_aspect=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): List of dicts with 'tokens', 'aspect', and 'polarity'.\n",
    "            label_map (dict): Mapping of sentiment labels to integers.\n",
    "            bert_model (str): Pretrained BERT model name.\n",
    "            max_len (int): Max sequence length for tokenization.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.label_map = label_map\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model)\n",
    "        self.max_len_sentence = max_len_sentence\n",
    "        self.max_len_aspect = max_len_aspect\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        sentence = \" \".join(item[\"tokens\"])  # Convert tokens to a full sentence\n",
    "        aspect = \" \".join(item[\"aspect_term\"])\n",
    "        polarity = self.label_map[item[\"polarity\"]]\n",
    "\n",
    "        # Tokenize sentence\n",
    "        sentence_inputs = self.tokenizer(sentence, padding=\"max_length\", truncation=True,\n",
    "                                         max_length=self.max_len_sentence, return_tensors=\"pt\")\n",
    "        \n",
    "        # Tokenize aspect\n",
    "        aspect_inputs = self.tokenizer(aspect, padding=\"max_length\", truncation=True,\n",
    "                                       max_length=self.max_len_aspect, return_tensors=\"pt\")\n",
    "\n",
    "        # Compute BERT embeddings (disable gradients for efficiency)\n",
    "        with torch.no_grad():\n",
    "            sentence_embedding = self.bert_model(**sentence_inputs).last_hidden_state.squeeze(0)  # [seq_len, emb_dim]\n",
    "            aspect_embedding = self.bert_model(**aspect_inputs).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        return sentence_embedding, aspect_embedding, torch.tensor(polarity, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSAModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_size=128, aspect_dim=768, num_lstm_layers=1, dropout = 0.5):\n",
    "        super(ABSAModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # GRU (Processes BERT word embeddings)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=num_lstm_layers, batch_first=True)\n",
    "        # Attention mechanism (Concatenating hidden states with aspect embeddings)\n",
    "        self.attention = nn.Linear(hidden_size + aspect_dim, 1, bias=False)  # GRU outputs 2*hidden_size\n",
    "\n",
    "        # Transformation Layer Before Softmax\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Final output layer (4 sentiment classes: pos, neg, neutral, conflict)\n",
    "        self.fc_output = nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, word_embeddings, aspect_embeddings):\n",
    "        \"\"\"\n",
    "        sentences: [batch_size, seq_len]\n",
    "        aspects: [batch_size] (aspect terms as indices)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = word_embeddings.shape  \n",
    "\n",
    "        # GRU Processing\n",
    "        lstm_out, _ = self.gru(word_embeddings)  # Output shape: [batch_size, seq_len, hidden_size * 2]\n",
    "        # lstm_out = self.norm(lstm_out)\n",
    "        # Concatenate Aspect Embedding with GRU Hidden States\n",
    "        aspect_repeated = aspect_embeddings.expand(-1, seq_len, -1)\n",
    "        \n",
    "        # Concatenate GRU output with aspect embeddings\n",
    "        att_input = torch.cat([lstm_out, aspect_repeated], dim=-1)  # [batch_size, seq_len, hidden_size + aspect_dim]\n",
    "        \n",
    "        # Compute Aspect-aware Attention Scores\n",
    "        att_weights = torch.tanh(self.attention(att_input))  # [batch_size, seq_len, 1]\n",
    "        att_weights = torch.softmax(att_weights, dim=1)  # Normalize across sequence\n",
    "        \n",
    "        # Compute Weighted Sum of GRU Hidden States\n",
    "        weighted_sum = torch.sum(lstm_out * att_weights, dim=1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Fully Connected Transformation Layer\n",
    "        transformed_features = self.fc_hidden(weighted_sum)  # [batch_size, hidden_size // 2]\n",
    "        transformed_features = self.activation(transformed_features)  # Non-linearity\n",
    "        transformed_features = self.dropout(transformed_features)  \n",
    "        \n",
    "        # Sentiment Prediction (Softmax over Output)\n",
    "        output = self.fc_output(transformed_features)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, model, num_epochs, lr, device):\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for batch in train_dataloader:\n",
    "            tokens, aspect, labels = batch  # Extract batch components\n",
    "            tokens, aspect, labels = tokens.to(device), aspect.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(tokens, aspect)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)  # Get predicted class\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                tokens, aspect, labels = batch\n",
    "                tokens, aspect, labels = tokens.to(device), aspect.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(tokens, aspect)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'ABSAModel.pth')\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarities: ['negative', 'positive', 'neutral', 'conflict']\n",
      "Max Sentence Length: 35.0\n",
      "Max Aspect Length: 3.0\n"
     ]
    }
   ],
   "source": [
    "with open('train_task_2.json', 'r', encoding='utf-8') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "polarities = []\n",
    "token_lengths = []\n",
    "aspect_lengths = []\n",
    "for item in train_data:\n",
    "    if(item['polarity'] not in polarities):\n",
    "        polarities.append(item['polarity'])\n",
    "    token_lengths.append(len(item['tokens']))\n",
    "    aspect_lengths.append(len(item['aspect_term']))\n",
    "    \n",
    "print(\"polarities:\", polarities)\n",
    "print(\"Max Sentence Length:\", np.percentile(token_lengths, 95))\n",
    "print(\"Max Aspect Length:\", np.percentile(aspect_lengths, 95))\n",
    "\n",
    "\n",
    "with open('val_task_2.json', 'r', encoding='utf-8') as file:\n",
    "    val_data = json.load(file)\n",
    "    \n",
    "label_map = {\"positive\": 0, \"negative\": 1, \"neutral\": 2, \"conflict\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "absa_model = ABSAModel()\n",
    "train_dataset = ABSADataset(train_data, label_map)\n",
    "val_dataset = ABSADataset(val_data, label_map)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# absa_train_loss_list, absa_val_loss_list = train(train_dataloader, val_dataloader, absa_model, num_epochs=10, lr=1e-3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(items, title, color):\n",
    "    epochs = list(range(1, len(items) + 1))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, items, color=color, linestyle='-', linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(absa_train_loss_list, \"Architecture Train Loss\", \"blue\")\n",
    "# plot_loss(absa_val_loss_list, \"Architecture Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_architecture(data_path, model_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        test_data = json.load(file)\n",
    "    test_dataset = ABSADataset(test_data, label_map)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = ABSAModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            tokens, aspect, labels = batch\n",
    "            tokens, aspect, labels = tokens.to(device), aspect.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(tokens, aspect)\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "    print(f\"Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# test_architecture('val_task_2.json', 'ABSAModel.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label_map, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): List of dicts with 'tokens', 'aspect', and 'polarity'.\n",
    "            label_map (dict): Mapping of sentiment labels to integers.\n",
    "            tokenizer: BERT tokenizer\n",
    "            max_length: Maximum tokenized sequence length\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.label_map = label_map\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sentence = \" \".join(sample[\"tokens\"])  # Convert token list to string\n",
    "        aspect = \" \".join(sample[\"aspect_term\"])  # Convert aspect list to string\n",
    "        polarity = self.label_map[sample[\"polarity\"]]  # Convert polarity to label index\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            sentence, aspect,\n",
    "            padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)  # Shape: (max_length,)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)  # Shape: (max_length,)\n",
    "        label = torch.tensor(polarity, dtype=torch.long)  # Scalar tensor\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=4):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BARTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, model_name='facebook/bart-base'):\n",
    "        super(BARTClassifier, self).__init__()\n",
    "        self.bart = BartModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bart.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.fc(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, model_name='roberta-base'):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "        self.roberta = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        \n",
    "        # Extract the last hidden state if needed\n",
    "        hidden_states = outputs.hidden_states  # Tuple of all layers\n",
    "        cls_output = hidden_states[-1][:, 0, :]  # CLS token from last layer\n",
    "        \n",
    "        return outputs.logits  # Return model's logits for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, tokenizer, path):\n",
    "    train_dataset = SentimentDataset(train_data, label_map, tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataset = SentimentDataset(val_data, label_map, tokenizer)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Optimizer and Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, dim=1)  # Get predicted class\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits, dim=1)  # Get predicted class\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "            \n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "            \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = BERTClassifier(num_labels=4).to(device)\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# bert_train_loss_list, bert_val_loss_list = finetune(bert_model, bert_tokenizer, \"finetunedBERT.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(bert_train_loss_list, \"Finetuned BERT Train Loss\", \"blue\")\n",
    "# plot_loss(bert_val_loss_list, \"Finetuned BERT Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bart_model = BARTClassifier(num_labels=4).to(device)\n",
    "# bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "# bart_train_loss_list, bart_val_loss_list = finetune(bart_model, bart_tokenizer, \"finetunedBART.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(bart_train_loss_list, \"Finetuned BART Train Loss\", \"blue\")\n",
    "# plot_loss(bart_val_loss_list, \"Finetuned BART Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta_model = RoBERTaClassifier(num_labels=4).to(device)\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# roberta_train_loss_list, roberta_val_loss_list = finetune(roberta_model, roberta_tokenizer, \"finetunedRoBERTa.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_loss(roberta_train_loss_list, \"Finetuned RoBERTa Train Loss\", \"blue\")\n",
    "# plot_loss(roberta_val_loss_list, \"Finetuned RoBERTa Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_finetuned(data_path, model_class, tokenizer, model_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        test_data = json.load(file)\n",
    "    test_dataset = SentimentDataset(test_data, label_map, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    model = model_class(num_labels=4)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            _, predicted = torch.max(logits, dim=1)  # Get predicted class\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "        \n",
    "    test_acc = test_correct / test_total\n",
    "    print(f\"finetuned {model_path[9:-4]} Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# test_finetuned('val_task_2.json', BERTClassifier, bert_tokenizer, 'finetunedBERT.pth')\n",
    "# test_finetuned('val_task_2.json', BARTClassifier, bart_tokenizer, 'finetunedBART.pth')\n",
    "# test_finetuned('val_task_2.json', RoBERTaClassifier, roberta_tokenizer, 'finetunedRoBERTa.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.57%\n",
      "finetuned BERT Accuracy: 77.38%\n",
      "finetuned BART Accuracy: 79.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetuned RoBERTa Accuracy: 78.75%\n"
     ]
    }
   ],
   "source": [
    "preprocessing('test.json', 'test_task_2.json')\n",
    "test_architecture('test_task_2.json', 'ABSAModel.pth')\n",
    "\n",
    "test_finetuned('test_task_2.json', BERTClassifier, bert_tokenizer, 'finetunedBERT.pth')\n",
    "test_finetuned('test_task_2.json', BARTClassifier, bart_tokenizer, 'finetunedBART.pth')\n",
    "test_finetuned('test_task_2.json', RoBERTaClassifier, roberta_tokenizer, 'finetunedRoBERTa.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 8.12 MB\n",
      "Cached: 1116.00 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "print(f\"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
