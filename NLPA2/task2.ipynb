{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BartTokenizer, BartModel, RobertaTokenizer, RobertaModel\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_index(tokens, start):\n",
    "    char_index = 0\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        token_start = char_index\n",
    "        token_end = char_index + len(token)\n",
    "        \n",
    "        if token_start <= start and start <= token_end:\n",
    "            return i\n",
    "        \n",
    "        char_index = token_end + 1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(input_file, output_file):  \n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for item in data:\n",
    "        sentence = item['sentence']\n",
    "        tokens = sentence.split()\n",
    "        aspect_terms = item['aspect_terms']\n",
    "\n",
    "        for aspect_term in aspect_terms:\n",
    "            start = int(aspect_term['from'])\n",
    "            end = int(aspect_term['to'])\n",
    "            index = get_token_index(tokens, start)\n",
    "\n",
    "            preprocessed_item = {\n",
    "                'tokens': tokens,\n",
    "                'polarity': aspect_term['polarity'],\n",
    "                'aspect_term': aspect_term['term'].split(),\n",
    "                'index': index\n",
    "            }\n",
    "            preprocessed_data.append(preprocessed_item)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(preprocessed_data, out_file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "preprocessing('train.json', 'train_task_2.json')\n",
    "preprocessing('val.json', 'val_task_2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, data, label_map, bert_model=\"bert-base-uncased\", max_len=35):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): List of dicts with 'tokens', 'aspect', and 'polarity'.\n",
    "            label_map (dict): Mapping of sentiment labels to integers.\n",
    "            bert_model (str): Pretrained BERT model name.\n",
    "            max_len (int): Max sequence length for tokenization.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.label_map = label_map\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        sentence = \" \".join(item[\"tokens\"])  # Convert tokens to a full sentence\n",
    "        aspect = \" \".join(item[\"aspect_term\"])\n",
    "        polarity = self.label_map[item[\"polarity\"]]\n",
    "\n",
    "        # Tokenize sentence\n",
    "        sentence_inputs = self.tokenizer(sentence, padding=\"max_length\", truncation=True,\n",
    "                                         max_length=self.max_len, return_tensors=\"pt\")\n",
    "        \n",
    "        # Tokenize aspect\n",
    "        aspect_inputs = self.tokenizer(aspect, padding=\"max_length\", truncation=True,\n",
    "                                       max_length=self.max_len, return_tensors=\"pt\")\n",
    "\n",
    "        # Compute BERT embeddings (disable gradients for efficiency)\n",
    "        with torch.no_grad():\n",
    "            sentence_embedding = self.bert_model(**sentence_inputs).last_hidden_state.squeeze(0)  # [seq_len, emb_dim]\n",
    "            aspect_embedding = self.bert_model(**aspect_inputs).last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        return sentence_embedding, aspect_embedding, torch.tensor(polarity, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABSAModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=768, hidden_size=128, aspect_dim=768, num_lstm_layers=1, dropout = 0.5):\n",
    "        super(ABSAModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # GRU (Processes BERT word embeddings)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=num_lstm_layers, batch_first=True)\n",
    "        # Attention mechanism (Concatenating hidden states with aspect embeddings)\n",
    "        self.attention = nn.Linear(hidden_size + aspect_dim, 1, bias=False)  # GRU outputs 2*hidden_size\n",
    "\n",
    "        # Transformation Layer Before Softmax\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        # Final output layer (4 sentiment classes: pos, neg, neutral, conflict)\n",
    "        self.fc_output = nn.Linear(hidden_size, 4)\n",
    "\n",
    "    def forward(self, word_embeddings, aspect_embeddings):\n",
    "        \"\"\"\n",
    "        sentences: [batch_size, seq_len]\n",
    "        aspects: [batch_size] (aspect terms as indices)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = word_embeddings.shape  \n",
    "\n",
    "        # GRU Processing\n",
    "        lstm_out, _ = self.gru(word_embeddings)  # Output shape: [batch_size, seq_len, hidden_size * 2]\n",
    "        lstm_out = self.norm(lstm_out)\n",
    "        # Concatenate Aspect Embedding with GRU Hidden States\n",
    "        aspect_repeated = aspect_embeddings.expand(-1, seq_len, -1)\n",
    "        \n",
    "        # Concatenate GRU output with aspect embeddings\n",
    "        att_input = torch.cat([lstm_out, aspect_repeated], dim=-1)  # [batch_size, seq_len, hidden_size + aspect_dim]\n",
    "        \n",
    "        # Compute Aspect-aware Attention Scores\n",
    "        att_weights = torch.tanh(self.attention(att_input))  # [batch_size, seq_len, 1]\n",
    "        att_weights = torch.softmax(att_weights, dim=1)  # Normalize across sequence\n",
    "        \n",
    "        # Compute Weighted Sum of GRU Hidden States\n",
    "        weighted_sum = torch.sum(lstm_out * att_weights, dim=1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Fully Connected Transformation Layer\n",
    "        transformed_features = self.fc_hidden(weighted_sum)  # [batch_size, hidden_size // 2]\n",
    "        transformed_features = self.activation(transformed_features)  # Non-linearity\n",
    "        transformed_features = self.dropout(transformed_features)  \n",
    "        \n",
    "        # Sentiment Prediction (Softmax over Output)\n",
    "        output = self.fc_output(transformed_features)  # [batch_size, output_dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, val_dataloader, model, num_epochs, lr, device):\n",
    "    \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for batch in train_dataloader:\n",
    "            tokens, aspect, labels = batch  # Extract batch components\n",
    "            tokens, aspect, labels = tokens.to(device), aspect.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(tokens, aspect)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Track metrics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)  # Get predicted class\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                tokens, aspect, labels = batch\n",
    "                tokens, aspect, labels = tokens.to(device), aspect.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(tokens, aspect)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, dim=1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'ABSAModel.pth')\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarities: ['negative', 'positive', 'neutral', 'conflict']\n",
      "Max Sentence Length: 35.0\n",
      "Max Aspect Length: 3.0\n"
     ]
    }
   ],
   "source": [
    "with open('train_task_2.json', 'r', encoding='utf-8') as file:\n",
    "    train_data = json.load(file)\n",
    "\n",
    "polarities = []\n",
    "token_lengths = []\n",
    "aspect_lengths = []\n",
    "for item in train_data:\n",
    "    if(item['polarity'] not in polarities):\n",
    "        polarities.append(item['polarity'])\n",
    "    token_lengths.append(len(item['tokens']))\n",
    "    aspect_lengths.append(len(item['aspect_term']))\n",
    "    \n",
    "print(\"polarities:\", polarities)\n",
    "print(\"Max Sentence Length:\", np.percentile(token_lengths, 95))\n",
    "print(\"Max Aspect Length:\", np.percentile(aspect_lengths, 95))\n",
    "\n",
    "\n",
    "with open('val_task_2.json', 'r', encoding='utf-8') as file:\n",
    "    val_data = json.load(file)\n",
    "    \n",
    "label_map = {\"positive\": 0, \"negative\": 1, \"neutral\": 2, \"conflict\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "absa_model = ABSAModel()\n",
    "train_dataset = ABSADataset(train_data, label_map)\n",
    "val_dataset = ABSADataset(val_data, label_map)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.9647, Train Acc: 0.6673 | Val Loss: 0.9572, Val Acc: 0.6765\n",
      "Epoch [2/10] - Train Loss: 0.8727, Train Acc: 0.7264 | Val Loss: 0.9555, Val Acc: 0.6550\n",
      "Epoch [3/10] - Train Loss: 0.8241, Train Acc: 0.7491 | Val Loss: 0.9375, Val Acc: 0.6765\n",
      "Epoch [4/10] - Train Loss: 0.7993, Train Acc: 0.7646 | Val Loss: 0.9748, Val Acc: 0.6523\n",
      "Epoch [5/10] - Train Loss: 0.7880, Train Acc: 0.7649 | Val Loss: 0.9812, Val Acc: 0.6712\n",
      "Epoch [6/10] - Train Loss: 0.7615, Train Acc: 0.7862 | Val Loss: 1.0056, Val Acc: 0.6631\n",
      "Epoch [7/10] - Train Loss: 0.7433, Train Acc: 0.7855 | Val Loss: 0.9753, Val Acc: 0.6712\n",
      "Epoch [8/10] - Train Loss: 0.7219, Train Acc: 0.8021 | Val Loss: 1.0137, Val Acc: 0.6442\n",
      "Epoch [9/10] - Train Loss: 0.7139, Train Acc: 0.8075 | Val Loss: 0.9880, Val Acc: 0.6765\n",
      "Epoch [10/10] - Train Loss: 0.7054, Train Acc: 0.8159 | Val Loss: 1.0412, Val Acc: 0.6658\n"
     ]
    }
   ],
   "source": [
    "absa_train_loss_list, absa_val_loss_list = train(train_dataloader, val_dataloader, absa_model, num_epochs=10, learning_rate=1e-3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'absa_train_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m)\n\u001b[0;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()  \n\u001b[1;32m---> 14\u001b[0m plot_loss(\u001b[43mabsa_train_loss_list\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m plot_loss(absa_val_loss_list, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'absa_train_loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_loss(items, title, color):\n",
    "    epochs = list(range(1, len(items) + 1))\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, items, color=color, linestyle='-', linewidth=2)\n",
    "    \n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.xticks(epochs)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()  \n",
    "\n",
    "plot_loss(absa_train_loss_list, \"Architecture Train Loss\", \"blue\")\n",
    "plot_loss(absa_val_loss_list, \"Architecture Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.6550%\n"
     ]
    }
   ],
   "source": [
    "def test_architecture(data_path, model_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        test_data = json.load(file)\n",
    "    test_dataset = ABSADataset(test_data, label_map)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = ABSAModel()\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            tokens, aspect, labels = batch\n",
    "            tokens, aspect, labels = tokens.to(device), aspect.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(tokens, aspect)\n",
    "\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_acc = test_correct / test_total\n",
    "    print(f\"Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "test_architecture('val_task_2.json', 'ABSA.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, label_map, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (list): List of dicts with 'tokens', 'aspect', and 'polarity'.\n",
    "            label_map (dict): Mapping of sentiment labels to integers.\n",
    "            tokenizer: BERT tokenizer\n",
    "            max_length: Maximum tokenized sequence length\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.label_map = label_map\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sentence = \" \".join(sample[\"tokens\"])  # Convert token list to string\n",
    "        aspect = \" \".join(sample[\"aspect_term\"])  # Convert aspect list to string\n",
    "        polarity = self.label_map[sample[\"polarity\"]]  # Convert polarity to label index\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            sentence, aspect,\n",
    "            padding=\"max_length\", truncation=True, max_length=self.max_length, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)  # Shape: (max_length,)\n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)  # Shape: (max_length,)\n",
    "        label = torch.tensor(polarity, dtype=torch.long)  # Scalar tensor\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=4):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BARTClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, model_name='facebook/bart-base'):\n",
    "        super(BARTClassifier, self).__init__()\n",
    "        self.bart = BartModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.bart.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.fc(cls_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, model_name='roberta-base'):\n",
    "        super(RoBERTaClassifier, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token representation\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.fc(cls_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, tokenizer, path):\n",
    "    train_dataset = SentimentDataset(train_data, label_map, tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_dataset = SentimentDataset(val_data, label_map, tokenizer)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Optimizer and Loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "        for batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits, dim=1)  # Get predicted class\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits, dim=1)  # Get predicted class\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "            \n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        val_loss_list.append(avg_val_loss)\n",
    "            \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), path)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return train_loss_list, val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERTClassifier(num_labels=4).to(device)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_train_loss_list, bert_val_loss_list = finetune(bert_model, bert_tokenizer, \"checkBERT.pth\")\n",
    "plot_loss(bert_train_loss_list, \"Finetuned BERT Train Loss\", \"blue\")\n",
    "plot_loss(bert_val_loss_list, \"Finetuned BERT Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65467ef97d584a1abea49b4b3aacd1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Aditya\\.cache\\huggingface\\hub\\models--facebook--bart-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f54f675c464962a9ec11d4b140a254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd1edce582d49ae905bb75ddd03f9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2955d214045c432f8272f24e6687fdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e70bd73d1f4ef38b09cadd4ab5f836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] - Train Loss: 1.1468, Train Acc: 0.5410 | Val Loss: 1.0562, Val Acc: 0.6415\n",
      "Epoch [2/3] - Train Loss: 0.8396, Train Acc: 0.6829 | Val Loss: 0.8511, Val Acc: 0.6792\n",
      "Epoch [3/3] - Train Loss: 0.6993, Train Acc: 0.7420 | Val Loss: 0.8215, Val Acc: 0.7116\n"
     ]
    }
   ],
   "source": [
    "bart_model = BARTClassifier(num_labels=4).to(device)\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "bart_train_loss_list, bart_val_loss_list = finetune(bart_model, bart_tokenizer, \"finetunedBART.pth\")\n",
    "plot_loss(bart_train_loss_list, \"Finetuned BART Train Loss\", \"blue\")\n",
    "plot_loss(bart_val_loss_list, \"Finetuned BART Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.8698, Train Acc: 0.6535 | Val Loss: 0.7806, Val Acc: 0.7197\n",
      "Epoch [2/10] - Train Loss: 0.6059, Train Acc: 0.7676 | Val Loss: 0.6770, Val Acc: 0.7278\n",
      "Epoch [3/10] - Train Loss: 0.4655, Train Acc: 0.8237 | Val Loss: 0.8093, Val Acc: 0.7412\n",
      "Epoch [4/10] - Train Loss: 0.3232, Train Acc: 0.8845 | Val Loss: 0.7023, Val Acc: 0.7520\n",
      "Epoch [5/10] - Train Loss: 0.2319, Train Acc: 0.9156 | Val Loss: 0.8088, Val Acc: 0.7709\n",
      "Epoch [6/10] - Train Loss: 0.1544, Train Acc: 0.9450 | Val Loss: 0.8991, Val Acc: 0.7682\n",
      "Epoch [7/10] - Train Loss: 0.1105, Train Acc: 0.9642 | Val Loss: 1.0019, Val Acc: 0.7601\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m roberta_model \u001b[38;5;241m=\u001b[39m RoBERTaClassifier(num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m roberta_tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m roberta_train_loss_list, roberta_val_loss_list \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroberta_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroberta_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckRoBERTa.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 31\u001b[0m, in \u001b[0;36mfinetune\u001b[1;34m(model, tokenizer, path)\u001b[0m\n\u001b[0;32m     28\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask)\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     34\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Aditya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "roberta_model = RoBERTaClassifier(num_labels=4).to(device)\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_train_loss_list, roberta_val_loss_list = finetune(roberta_model, roberta_tokenizer, \"checkRoBERTa.pth\")\n",
    "plot_loss(roberta_train_loss_list, \"Finetuned RoBERTa Train Loss\", \"blue\")\n",
    "plot_loss(roberta_val_loss_list, \"Finetuned RoBERTa Val Loss\", \"orange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.32%\n",
      "Accuracy: 71.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.01%\n"
     ]
    }
   ],
   "source": [
    "def test_finetuned(data_path, model_class, tokenizer, model_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        test_data = json.load(file)\n",
    "    test_dataset = SentimentDataset(test_data, label_map, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    model = model_class(num_labels=4)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            _, predicted = torch.max(logits, dim=1)  # Get predicted class\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "        \n",
    "    test_acc = test_correct / test_total\n",
    "    print(f\"Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "test_finetuned('val_task_2.json', BERTClassifier, bert_tokenizer, 'finetunedBERT.pth')\n",
    "test_finetuned('val_task_2.json', BARTClassifier, bart_tokenizer, 'finetunedBART.pth')\n",
    "test_finetuned('val_task_2.json', RoBERTaClassifier, roberta_tokenizer, 'finetunedRoBERTa.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
